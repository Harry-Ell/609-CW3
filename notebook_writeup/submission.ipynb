{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcf7044",
   "metadata": {},
   "source": [
    "# Submission for Coursework 3\n",
    "Team: Billie-Jo Powers, Jasmine Burgess, $\\tiny{Harry}$ & Mark Holcroft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a196057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb58589a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf58820",
   "metadata": {},
   "source": [
    "This will contain a small exposition of what we are hoping to do. Probably going to wait until this is done in the report to drag and drop in here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1dcacf",
   "metadata": {},
   "source": [
    "## Simplified Game of Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3f9cb",
   "metadata": {},
   "source": [
    "some exposition about the game of pig and why it is simpler. Say that we are about to make the same thing as figure 2 in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a92c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from piglet import PigletSolver\n",
    "piglet_plotter = PigletSolver(goal = 2, epsilon = 10e-6)\n",
    "piglet_plotter(convergence_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2465ca5",
   "metadata": {},
   "source": [
    "talk about how early states have to wait for later ones to converge. Hence this motivates other procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b1621",
   "metadata": {},
   "source": [
    "## Surface Plot of Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f08f9",
   "metadata": {},
   "source": [
    "we now apply this to the next section which is the full game of pig. This starts by remaking plots 3 from the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a856df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to pre complilation of loops using numba package, the optimal policy can be derived in minimal time\n",
    "from notebook_writeup.optimised_layered_vi import pig_layered_value_iteration\n",
    " \n",
    "die_size = 6\n",
    "target_score = 100\n",
    "max_turn = 100\n",
    "values, policy = pig_layered_value_iteration(target_score=target_score, \n",
    "                                             die_sides=die_size, \n",
    "                                             max_turn=max_turn, \n",
    "                                             epsilon=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can display our policy using the following plotting module\n",
    "from notebook_writeup.plotting_tools import surface_plotter\n",
    "surface_plotter(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b14a0f",
   "metadata": {},
   "source": [
    "### Extracting Reachable States "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25936bcb",
   "metadata": {},
   "source": [
    "this is most easily done by exact simulation in my opinion. Thats how i did it, and thats the file I read in here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088fd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reachable states are extractable from experiment\n",
    "with open('pickle_and_config_files/reachable_states.pkl', 'rb') as d:\n",
    "    reachable_states = pickle.load(d)\n",
    "surface_plotter(reachable_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57bf33",
   "metadata": {},
   "source": [
    "below we will have a cross section of this which is meant to look like the one from the lecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_section_reachable = np.array([[reachable_states[(i, 30, k)] for i in range(101)] for k in range(100, -1, -1)])\n",
    "cross_section_policy = policy[:, 30, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Show the two imshow layers\n",
    "plt.imshow(cross_section_policy.T, cmap='Blues', alpha=0.6)\n",
    "plt.imshow(cross_section_reachable, cmap='Reds', alpha=0.4)\n",
    "\n",
    "# Axis labels\n",
    "plt.xlabel(\"Player Score\")\n",
    "plt.ylabel(\"Opponent Score\")\n",
    "\n",
    "# Custom legend using colored patches\n",
    "policy_patch = mpatches.Patch(color='blue', alpha=0.6, label='Policy Region')\n",
    "reachable_patch = mpatches.Patch(color='red', alpha=0.4, label='Reachable Region')\n",
    "plt.legend(handles=[policy_patch, reachable_patch], loc='upper right')\n",
    "\n",
    "plt.title(\"Policy vs Reachable State Overlay\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3b8ca",
   "metadata": {},
   "source": [
    "notes on the above plot: I think it looks pretty shit. I will be rerunning the code which maps the state space for many more iterations. Hopefully that will give us a lot more smoothness on the red region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb6f26",
   "metadata": {},
   "source": [
    "### Contours of winning probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fe40b",
   "metadata": {},
   "source": [
    "Small talk about what this is etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822209f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make use of our final values outputted by the \n",
    "from notebook_writeup.plotting_tools import plot_isosurface_from_array\n",
    "\n",
    "plot_isosurface_from_array(values, isovalues=[0.03, 0.09, 0.27, 0.81])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961a99f",
   "metadata": {},
   "source": [
    "### Reproducing Score Analytics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4827f38e",
   "metadata": {},
   "source": [
    "what a shit title that defo needs to change btw, but that is what we settle on for now \n",
    "\n",
    "I must also confess that i changed your code to make it so the competition code will read from numpy arrays rather than the dictionaries. Bit of a pisstake i know, but the pickle dicts served their time well during development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0104a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In section 'the solution' they rattle off some %s for wins and stuff. we will try and reproduce them here \n",
    "from competition import Competition\n",
    "competition_function = Competition(policy, policy, replications=100000, seed = 1)\n",
    "win_percentage = competition_function()\n",
    "print(f'For two optimal policies playing against each other, player 1 wins {round(win_percentage*100, 3)}% of the time.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this win percentage is the same as given in the value function\n",
    "float(values[0,0,0] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6027722",
   "metadata": {},
   "source": [
    "next we want something like this but for playing against the hold at 20 player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have another class called opponents which can make policies for us to play against if we want to \n",
    "from competition import Opponents\n",
    "hold_at_20_policy = Opponents.hold_at_20()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_function_h20 = Competition(policy, hold_at_20_policy, replications=100000, seed = 1)\n",
    "win_percentage_h20 = competition_function_h20()\n",
    "print(f'For optimal policy playing against hold at 20, optimal player wins {round(win_percentage_h20*100, 3)}% of the time.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
