{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcf7044",
   "metadata": {},
   "source": [
    "# Submission for Coursework 3\n",
    "Team: Billie-Jo Powers, Jasmine Burgess, Mark Holcroft & $\\tiny{Harry Ellington}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a196057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb58589a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf58820",
   "metadata": {},
   "source": [
    "Pig is a simple dice game of chance, whose interesting probabilistic features have been well‐studied in a vast number of academic papers. This report re‐examines such a paper by Todd W. Nellor and Clifton G.M. Presser. The aim of their paper is to find the optimal policy a player of the game Pig should use, and they calculate this using the value iteration algorithm. At the time of the publishing of the paper, classical value iteration was seen as too slow and states took too long to converge, and hence a ‘layered’ approach of working backwards was used. This works by using value iteration on subsets of the state space, ensuring these converge before moving onto the next subset. The paper found that the optimal policy is non‐smooth and includes unusual and unintuitive features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1dcacf",
   "metadata": {},
   "source": [
    "## Simplified Game of Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3f9cb",
   "metadata": {},
   "source": [
    "Neller and Presser initially work with a simplified version of the game Pig, named \"Piglet\". In this version, we replace the dice with a coin, where landing on tails is the equivalent of rolling a one. Traditionally, the goal of Piglet is to reach 10 points first; however, the number of equations required to calculate the proability of winning from each possible state is still too large for a simple example, so we instead set a goal of 2. \n",
    "\n",
    "Letting $P_{i,j,k}$ be the probability of the player winning given that their score is $i$, the opponents score is $j$, and the current turn total is $k$, below we recreate figure $2$ from Neller and Presser, which shows the result of applying value iteration to Piglet with a goal score of $2$. We were able to confirm that our results gave the same final probabilities as Neller and Presser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a92c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from piglet import PigletSolver\n",
    "piglet_plotter = PigletSolver(goal = 2, epsilon = 10e-6)\n",
    "piglet_plotter(convergence_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2465ca5",
   "metadata": {},
   "source": [
    "#talk about how early states have to wait for later ones to converge. Hence this motivates other procedure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b1621",
   "metadata": {},
   "source": [
    "## Surface Plot of Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f08f9",
   "metadata": {},
   "source": [
    "We are now able to look at reproducing the result given for the game of Pig, as with Piglet, layered value iteration is used to find an optimal policy for playing the game of Pig. Here, we are working with a six sided dice and a goal score of $100$, as in the traditional rules, though our code used is capable of adapting the rules to user preference. Figure 3 in Neller and Presser gives a plot of the optimal policy, for which our recreation is given below. The interpretation of the graph is that for states below the surface, one should roll again, and for states above the surface, the optimal choice is to hold. \n",
    "\n",
    "Our recreation has distinct similarities to that produced by Neller and presser, which indicates a good level of reproduicibilty in this regard. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a856df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to pre complilation of loops using numba package, the optimal policy can be derived in minimal time\n",
    "from notebook_writeup.optimised_layered_vi import pig_layered_value_iteration\n",
    " \n",
    "die_size = 6\n",
    "target_score = 100\n",
    "max_turn = 100\n",
    "values, policy = pig_layered_value_iteration(target_score=target_score, \n",
    "                                             die_sides=die_size, \n",
    "                                             max_turn=max_turn, \n",
    "                                             epsilon=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can display our policy using the following plotting module\n",
    "from notebook_writeup.plotting_tools import generate_box_plots\n",
    "generate_box_plots(policy, title = 'Plot of optimal Policy', pad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b14a0f",
   "metadata": {},
   "source": [
    "### Extracting Reachable States "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25936bcb",
   "metadata": {},
   "source": [
    "Figures 5 and 6 of the original paper depict the optimal policy at all reachable states. Note here that the reachable states are calculated based on the assumption that the opponent could be playing using any strategy ‐ if they were to be playing optimally, these figures would be symmetric about the axis Player Score = Opponent Score. These figures have been reproduced below. They show clear similarities with those of the original paper, particularly the omission of states involving player scores of between 1 and 19, and the tendency to increase the maximum turn score for larger values of opponent score.\n",
    "\n",
    "Our recreation does exhibit slight deviations from Neller and Presser's results in regions where both players have high scores. This is likely due to those states being more difficult to reach, and so they appear less often in our simulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088fd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reachable states are extractable from experiment (PLACEHOLDER)\n",
    "with open('pickle_and_config_files/reachable_states.pkl', 'rb') as d:\n",
    "    reachable_array = pickle.load(d)\n",
    "generate_box_plots(reachable_array, title = 'Plot of Reachable States')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57bf33",
   "metadata": {},
   "source": [
    "Another graph which was successfully replicated was Figure 4. Shown in below, it depicts a cross‐section of the reachable states for an opponent score of 30, and includes the optimal boundary and the “hold at 20” heuristic as a benchmark for comparison. The graph was produced using simulation, and shares the same features and properties with that of the original paper. Note crucially that the reachable states are states which are reachable during any stage of the game ‐ not only when the opponent is on a score of 30. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_section_reachable = np.array([[reachable_array[(i, 30, k)] for i in range(101)] for k in range(100, -1, -1)])\n",
    "cross_section_policy = policy[:, 30, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Show the two imshow layers\n",
    "plt.imshow(cross_section_policy.T, cmap='Blues', alpha=0.6)\n",
    "plt.imshow(cross_section_reachable, cmap='Reds', alpha=0.4)\n",
    "\n",
    "# Axis labels\n",
    "plt.xlabel(\"Player Score\")\n",
    "plt.ylabel(\"Opponent Score\")\n",
    "\n",
    "# Custom legend using colored patches\n",
    "policy_patch = mpatches.Patch(color='blue', alpha=0.6, label='Policy Region')\n",
    "reachable_patch = mpatches.Patch(color='red', alpha=0.4, label='Reachable Region')\n",
    "plt.legend(handles=[policy_patch, reachable_patch], loc='upper right')\n",
    "\n",
    "plt.title(\"Policy vs Reachable State Overlay\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3b8ca",
   "metadata": {},
   "source": [
    "notes on the above plot: I think it looks pretty shit. I will be rerunning the code which maps the state space for many more iterations. Hopefully that will give us a lot more smoothness on the red region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb6f26",
   "metadata": {},
   "source": [
    "### Contours of winning probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fe40b",
   "metadata": {},
   "source": [
    "The final graph we reproduce from Teller and Presser is figure $7$, which gives the win probability contours for optimal play. This gives a visualisation of the probability of the optimal player winning given that they are in state $(i,j,k)$. In particular, this contnour gives the states for which the probability of winning is $3\\%, 9\\%, 27\\%$, and $81\\%$. Our reproduction of the graph is given below and is consistent with that found in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822209f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make use of our final values outputted by the \n",
    "from notebook_writeup.plotting_tools import plot_isosurface_from_array\n",
    "\n",
    "plot_isosurface_from_array(values, isovalues=[0.03, 0.09, 0.27, 0.81])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961a99f",
   "metadata": {},
   "source": [
    "### Reproducing Score Analytics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4827f38e",
   "metadata": {},
   "source": [
    "Neller and Presser conclude that in the scenario where both players are playing optimally, the player who goes first will win $53.06\\%$ of the time ( i.e. $P_{0,0,0} = 0.5306$). To reproduce these results, we used a simulation of the game pig, with 100,000 replications, with a starting seed of 123. The code used for these simulation is available in the files called below, and in this case give that the first player wins $53.047\\%$ of the games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0104a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In section 'the solution' they rattle off some %s for wins and stuff. we will try and reproduce them here \n",
    "from competition import Competition\n",
    "competition_function = Competition(policy, policy, replications=100000, seed = 123)\n",
    "win_percentage = competition_function()\n",
    "print(f'For two optimal policies playing against each other, player 1 wins {round(win_percentage*100, 3)}% of the time.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a74d671",
   "metadata": {},
   "source": [
    "We also note that our value $P_{0,0,0}$, as given below is $0.5306$, which is consistent with Neller and Pressers results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this win percentage is the same as given in the value function\n",
    "float(values[0,0,0] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6027722",
   "metadata": {},
   "source": [
    "Neller and Presser also reach some conclusion on what is expected when one player uses the optimal strategy and the other employs the \"hold at 20\" policy. To recreate these results, we again employed the simulation technique used above, where one player employs the \"hold at 20\" policy according to the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have another class called opponents which can make policies for us to play against if we want to \n",
    "from competition import Opponents\n",
    "hold_at_20_policy = Opponents.hold_at_n(20) # NOTE THAT YOU CAN TUNE THIS, WALK THEM THROUGH SOME OTHER VALS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef31c7",
   "metadata": {},
   "source": [
    "Neller and Presser conclude that when the optimal player goes first in this scenario, they should win $58.74\\%$ of the time, and when the \"hold at 20\" player goes first, they should win $47.76\\%$ of the time. Below is given out simulations of the same scenarios, again with 100,000 replications and a starting seed of 123. \n",
    "\n",
    "In our simulations, when the optimal player goes firt, we find that they win $57.21\\%$ of the games. Since this result is below what was expected from Neller and Pressers results, w include  confidence interval for this reult. **need to find the confidence interval code and insert it**. We also found in our simulation that when the hold at 20 player goes first, they win $49.518\\%$ fo the time which is again a slight deviation from the results found by Teller and Nelson, a confidence interval is again provided to assess this. \n",
    "\n",
    "**going to need to add more discussion on this with the confidence intervals as it seems we will have to conclude that we could not completely reproduce the results from the paper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_function_h20 = Competition(policy, hold_at_20_policy, replications=100000, seed = 10000)\n",
    "win_percentage_h20 = competition_function_h20()\n",
    "print(f'For optimal policy playing against hold at 20, optimal player wins {round(win_percentage_h20*100, 3)}% of the time.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deaa507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard error for \"h20_1\"\n",
    "se = np.sqrt(win_percentage_h20 * (1 - win_percentage_h20) / 100000)\n",
    "\n",
    "# Confidence interval for \"h_20_1\"\n",
    "ci_lower = win_percentage_h20 - 1.96 * se\n",
    "ci_upper = win_percentage_h20 + 1.96 * se\n",
    "\n",
    "print(f\"95% Confidence Interval: [{ci_lower:.3f}, {ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_function_h20_2 = Competition(hold_at_20_policy, policy, replications=100000, seed = 123)\n",
    "win_percentage_h20_2 = competition_function_h20_2()\n",
    "print(f'For optimal policy playing against hold at 20, h20 player wins {round(win_percentage_h20_2*100, 3)}% of the time.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
